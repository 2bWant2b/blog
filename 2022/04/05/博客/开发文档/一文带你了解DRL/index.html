<!DOCTYPE html><html lang="zh-CN"><head><!-- hexo injector head_begin start --><link href="https://cdn.jsdelivr.net/npm/hexo-tag-common@latest/css/index.css" rel="stylesheet"/><!-- hexo injector head_begin end --><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="theme-color" content="#0078E7"><meta name="author" content="2bW"><meta name="copyright" content="2bW"><meta name="generator" content="Hexo 5.3.0"><meta name="theme" content="hexo-theme-yun"><title>一文带你了解DRL | 2bWant2b</title><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ma+Shan+Zheng&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=ZCOOL+XiaoWei&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=ZCOOL+XiaoWei&amp;display=swap" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/star-markdown-css@0.1.24/dist/yun/yun-markdown.min.css"><script src="//at.alicdn.com/t/font_1140697_ed8vp4atwoj.js" async></script><script src="https://cdn.jsdelivr.net/npm/scrollreveal/dist/scrollreveal.min.js" defer></script><script>document.addEventListener("DOMContentLoaded", () => {
  [".post-card",".post-content img"].forEach((target)=> {
    ScrollReveal().reveal(target);
  })
});
</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.css"><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", function() {
  renderMathInElement(document.body, {
    delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false},
      {left: "\\(", right: "\\)", display: false},
      {left: "\\[", right: "\\]", display: true}
    ]
  });
});</script><link id="light-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism.css" media="(prefers-color-scheme: light)"><link id="dark-prism-css" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-tomorrow.css" media="(prefers-color-scheme: dark)"><link rel="shortcut icon" type="image/svg+xml" href="https://gitee.com/tzh363231879/picgo/raw/master/心favicon.png"><link rel="mask-icon" href="https://gitee.com/tzh363231879/picgo/raw/master/心favicon.png" color="#0078E7"><link rel="alternate icon" href="/yun.ico"><link rel="preload" href="/css/hexo-theme-yun.css" as="style"><link rel="preload" href="/js/utils.js" as="script"><link rel="preload" href="/js/hexo-theme-yun.js" as="script"><link rel="prefetch" href="/js/sidebar.js" as="script"><link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin><script id="yun-config">
    const Yun = window.Yun || {};
    window.CONFIG = {"hostname":"2bwant2b.github.io","root":"/","title":"回忆と珍藏","version":"1.5.1","mode":"time","copycode":true,"page":{"isPost":true},"anonymous_image":"https://cdn.jsdelivr.net/gh/YunYouJun/cdn/img/avatar/none.jpg","say":{"api":"https://v1.hitokoto.cn","hitokoto":true},"local_search":{"path":"/search.xml"},"fireworks":{"colors":["255, 182, 193","255, 192, 203","255, 240, 245"]}};
  </script><link rel="stylesheet" href="/css/hexo-theme-yun.css"><script src="/js/utils.js"></script><script src="/js/hexo-theme-yun.js"></script><script src="//at.alicdn.com/t/font_2367933_9jwlegvk82.js" async></script><script>CONFIG.leancloudVisitors = {"enable":true,"app_id":"P2B8WEYNUAQ0z68tTomJdPbP-MdYXbMMI","app_key":"iVlRH4IjcvkkJeAw7Eq2NuvU","server_url":"https://P2B8WEYN.api.lncldglobal.com"}</script><script defer src="/js/analytics/leancloud-visitors.js"></script><meta name="description" content="# 序言 本文源自 Medium 上的博主 Ketan Doshi（配图也源自于此），写的非常 nice，本文依照个人角度提炼了知识，有兴趣的小伙伴可以自行前往哦～ 本文包含以下内容：  Markov Decision Process Bellman Equation Model-free Solutions Q-Learning Deep Q Networks DDPG 更新中～  或许读完本文">
<meta property="og:type" content="article">
<meta property="og:title" content="一文带你了解DRL">
<meta property="og:url" content="https://2bwant2b.github.io/2022/04/05/%E5%8D%9A%E5%AE%A2/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3DRL/index.html">
<meta property="og:site_name" content="2bWant2b">
<meta property="og:description" content="# 序言 本文源自 Medium 上的博主 Ketan Doshi（配图也源自于此），写的非常 nice，本文依照个人角度提炼了知识，有兴趣的小伙伴可以自行前往哦～ 本文包含以下内容：  Markov Decision Process Bellman Equation Model-free Solutions Q-Learning Deep Q Networks DDPG 更新中～  或许读完本文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/robots.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/MDP.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Policy.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Policy2.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/State-Value.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Q-Value.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/OptimalPolicy.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Bellman1.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Bellman2.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Bellman3.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/policy-value-based.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/value-based.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/modelfree.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/RLSolution.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/policypick.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/greedy.jpg">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/PolicyImprove.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/ValueImprove.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/overview.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Frequency.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Depth.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/conclusion.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Q-Learning.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/Q-Function.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN1.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN2.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN3.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN0.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN4.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN5.png">
<meta property="og:image" content="https://gitee.com/tzh363231879/picgo/raw/master/DQN6.png">
<meta property="article:published_time" content="2022-04-05T06:39:49.000Z">
<meta property="article:modified_time" content="2022-04-05T14:23:55.556Z">
<meta property="article:author" content="2bW">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/tzh363231879/picgo/raw/master/robots.png"><script src="/js/ui/mode.js"></script></head><body><script defer src="https://cdn.jsdelivr.net/npm/animejs@latest"></script><script defer src="/js/ui/fireworks.js"></script><canvas class="fireworks"></canvas><div class="container"><a class="sidebar-toggle hty-icon-button" id="menu-btn"><div class="hamburger hamburger--spin" type="button"><span class="hamburger-box"><span class="hamburger-inner"></span></span></div></a><div class="sidebar-toggle sidebar-overlay"></div><aside class="sidebar"><script src="/js/sidebar.js"></script><ul class="sidebar-nav"><li class="sidebar-nav-item sidebar-nav-toc hty-icon-button sidebar-nav-active" data-target="post-toc-wrap" title="文章目录"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-list-ordered"></use></svg></li><li class="sidebar-nav-item sidebar-nav-overview hty-icon-button" data-target="site-overview-wrap" title="站点概览"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-passport-line"></use></svg></li></ul><div class="sidebar-panel" id="site-overview-wrap"><div class="site-info fix-top"><a class="site-author-avatar" href="/about/" title="2bW"><img width="96" loading="lazy" src="https://gitee.com/tzh363231879/picgo/raw/master/393x393_可爱.gif" alt="2bW"><span class="site-author-status" title="未来的总是美好的，相信未来">😊</span></a><div class="site-author-name"><a href="/about/">2bW</a></div><a class="site-name" href="/about/site.html">2bWant2b</a><sub class="site-subtitle">个人博客</sub><div class="site-desciption"></div></div><nav class="site-state"><a class="site-state-item hty-icon-button icon-home" href="/" title="首页"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-home-4-line"></use></svg></span></a><div class="site-state-item"><a href="/archives/" title="归档"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-archive-line"></use></svg></span><span class="site-state-item-count">46</span></a></div><div class="site-state-item"><a href="/categories/" title="分类"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-2-line"></use></svg></span><span class="site-state-item-count">5</span></a></div><div class="site-state-item"><a href="/tags/" title="标签"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="site-state-item-count">30</span></a></div><a class="site-state-item hty-icon-button" href="/bbs/" title="说说"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-open-arm-line"></use></svg></span></a></nav><hr style="margin-bottom:0.5rem"><div class="links-of-author"><a class="links-of-author-item hty-icon-button" rel="noopener" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=363231879&amp;website=www.oicqzone.com" title="QQ" target="_blank" style="color:#12B7F5"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-qq-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://github.com/2bWant2b" title="GitHub" target="_blank" style="color:#6e5494"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-github-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://music.163.com/#/user/home?id=249986987" title="网易云音乐" target="_blank" style="color:#C20C0C"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-netease-cloud-music-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://www.zhihu.com/people/2bwant2b/" title="知乎" target="_blank" style="color:#0084FF"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhihu-line"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="https://blog.csdn.net/tzh666777?spm=1000.2115.3001.5343" title="CSDN" target="_blank" style="color:rgb(244, 0, 0)"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-csdn"></use></svg></a><a class="links-of-author-item hty-icon-button" rel="noopener" href="mailto:tzh363231879@163.com" title="E-Mail" target="_blank" style="color:#8E71C1"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-mail-line"></use></svg></a></div><hr style="margin:0.5rem 1rem"><div class="links"><a class="links-item hty-icon-button" href="/links/" title="链接" style="color:red"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-share-line"></use></svg></a></div><br><a class="links-item hty-icon-button" id="toggle-mode-btn" href="javascript:;" title="Mode" style="color: #f1cb64"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-contrast-2-line"></use></svg></a></div><div class="sidebar-panel sidebar-panel-active" id="post-toc-wrap"><div class="post-toc"><div class="post-toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text"> 序言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#markov-decision-process"><span class="toc-number">2.</span> <span class="toc-text"> Markov Decision Process</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bellman-equation"><span class="toc-number">3.</span> <span class="toc-text"> Bellman Equation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-free-solutions"><span class="toc-number">4.</span> <span class="toc-text"> Model-free Solutions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#q-learning"><span class="toc-number">5.</span> <span class="toc-text"> Q-Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#deep-q-networks"><span class="toc-number">6.</span> <span class="toc-text"> Deep Q Networks</span></a></li></ol></div></div></div></aside><main class="sidebar-translate" id="content"><div id="post"><article class="hty-card post-block" itemscope itemtype="https://schema.org/Article"><link itemprop="mainEntityOfPage" href="https://2bwant2b.github.io/2022/04/05/%E5%8D%9A%E5%AE%A2/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3DRL/"><span hidden itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="name" content="2bW"><meta itemprop="description"></span><span hidden itemprop="publisher" itemscope itemtype="https://schema.org/Organization"><meta itemprop="name" content="2bWant2b"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">一文带你了解DRL</h1><div class="post-meta"><div class="post-time" style="display:block"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-calendar-line"></use></svg></span> <span class="post-meta-icon-text">发表于</span> <time title="创建时间：2022-04-05 14:39:49" itemprop="dateCreated datePublished" datetime="2022-04-05T14:39:49+08:00">2022-04-05</time></div><span class="post-count"><span class="post-symbolcount"><span class="post-meta-item-icon" title="本文字数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-file-word-line"></use></svg></span> <span title="本文字数">3.4k</span><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读时长"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-timer-line"></use></svg></span> <span title="阅读时长">13m</span></span></span><span class="post-busuanzi"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span id="busuanzi_value_page_pv"></span></span></span><span class="leancloud_visitors" id="/2022/04/05/%E5%8D%9A%E5%AE%A2/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3DRL/" data-flag-title="一文带你了解DRL"><span class="post-meta-divider">-</span><span class="post-meta-item-icon" title="阅读次数"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg> <span class="leancloud-visitors-count"></span></span></span><div class="post-classify"><span class="post-category"> <span class="post-meta-item-icon" style="margin-right:3px;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-folder-line"></use></svg></span><span itemprop="about" itemscope itemtype="https://schema.org/Thing"><a class="category" href="/categories/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/" style="--text-color:var(--hty-text-color)" itemprop="url" rel="index"><span itemprop="text">开发文档</span></a></span></span><span class="post-tag"><span class="post-meta-divider">-</span><a class="tag" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="--text-color:var(--hty-text-color)"><span class="post-meta-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-price-tag-3-line"></use></svg></span><span class="tag-name">深度学习</span></a></span></div></div></header><section class="post-body" itemprop="articleBody"><div class="post-content markdown-body" style="--smc-primary:#0078E7;"><h2 id="序言"><a class="markdownIt-Anchor" href="#序言">#</a> 序言</h2>
<p>本文源自 Medium 上的博主<a target="_blank" rel="noopener" href="https://medium.com/@ketanhdoshi"> Ketan Doshi</a>（配图也源自于此），写的非常 nice，本文依照个人角度提炼了知识，有兴趣的小伙伴可以自行前往哦～</p>
<p>本文包含以下内容：</p>
<ul>
<li><strong>Markov Decision Process</strong></li>
<li><strong>Bellman Equation</strong></li>
<li><strong>Model-free Solutions</strong></li>
<li><strong>Q-Learning</strong></li>
<li><strong>Deep Q Networks</strong></li>
<li><strong>DDPG 更新中～</strong></li>
</ul>
<p>或许读完本文你更能理解莫凡 PYTHON 的讲解</p>
<p>all right，带上你的脑子一起出发吧！</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/robots.png" alt="robots" loading="lazy"></p>
<h2 id="markov-decision-process"><a class="markdownIt-Anchor" href="#markov-decision-process">#</a> Markov Decision Process</h2>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/MDP.jpg" alt="MDP" loading="lazy"></p>
<p>为了使用 RL (Reinforcement Learning)，你需要将你的问题建模成为 MDP (Markov Decision Process)。MDP 包含以下五个部分：</p>
<p><strong>Agent</strong>：这是你想训练的一个载体，你可以把他想成一个机器人，你要训练他如何才能把你布置给他的任务做好</p>
<p><strong>Environment</strong>：环境是与 Agent 进行交互的地方，比如你想让机器人在工厂里拿去一个货物，那么这个工厂就是 Environment</p>
<p><strong>State</strong>：状态是环境和 Agent 在某一时刻的属性，比如机器人的行进速度、机器人在工厂的位置以及工厂内的风速、摩擦力等等</p>
<p><strong>Action</strong>：动作就是 Agent 以何种方式与环境进行交互，比如机器人可以前后左右运动等等</p>
<p><strong>Reward</strong>：当 Agent 在环境中采取动作以后，从环境中获得的奖励。这里的奖励可以是 “正奖励”，也可以是 “负奖励”。比如你的机器人向前走了一步之后撞到障碍物了，这就获得了 “负奖励”。而如果你的机器人拿到你想要的物品了，这就获得了 “正奖励”</p>
<p>那么现在你可能就会问：Agent 如何选取 Action？毕竟好的 Action 可以让 Agent 更高质量的完成任务，而这也是强化学习的终极目标。在回答这个问题之前，我们需要了解 3 个概念：Return、Policy、Value</p>
<p><strong>Return</strong>：回报是 Agent 在执行任务期间每一步所获得 Reward 的总和。我们并非简单地将这些 Reward 相加求和，而是用到了 discount factor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Return</mtext><mo>=</mo><msub><mi>r</mi><mn>0</mn></msub><mo>+</mo><mi>γ</mi><msub><mi>r</mi><mn>1</mn></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>r</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msup><mi>γ</mi><mi>n</mi></msup><msub><mi>r</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\text{Return} = r_0 + \gamma r_1 + \gamma^2 r_2 + \cdots \gamma^n r_n
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">Return</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0585479999999998em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.9088319999999999em;vertical-align:-0.19444em;"></span><span class="minner">⋯</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>注意这里的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo>&lt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\gamma &lt;1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，这就表明了 Agent 其实更加注重当前的 Reward <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">r_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。其次，通过定义 Return 让 Agent 放长眼光，要综合考虑长时间段的步骤之后来选取 Action。</p>
<p><strong>Policy</strong>：Policy 是 Agent 在当前 Current State 下选取 Action 时所遵循的一种策略。如随机选取动作、选取已知给予最高 reward 的动作等等。这初听起来比较抽象，其实 Policy 就像是一张查找表（状态数有限）或者是一个 Function（状态数非常多），它给出了给定状态下选取某个动作的概率，可记为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo stretchy="false">(</mo><msub><mi>a</mi><mn>2</mn></msub><mi mathvariant="normal">∣</mi><msub><mi>S</mi><mn>3</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\pi(a_2|S_3)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，即当前状态为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>S</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">S_3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，依照 Policy 此时选取动作<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">a_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的概率。</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Policy.jpg" alt="Policy" loading="lazy"></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Policy2.jpg" alt="Policy2" loading="lazy"></p>
<p>我们可以选取非常多种 Policy，但是哪一种是可以最大化 Return 的？这就需要我们了解第三个概念 Value</p>
<p><strong>Value</strong>：我们假设 Agent 每次到达同一个 State 时，都遵循相同的 Policy <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> 选取 Action，那么经过很多次的迭代之后，average long-term Return 或者说 expected Return 被称为 Value。说的直白一点，Value 是你对 Return 的一个预测，一个预期。Return 是一个 episode 内实实在在得到的，Value 是经过很多 episodes 后的期望 Return，而这个预测是要不断修正的，这个后面会提到。</p>
<p>我们有两种 Value：State Value 和 State-Action Value (Q-Value)</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/State-Value.jpg" alt="State-Value" loading="lazy"></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Q-Value.jpg" alt="Q-Value" loading="lazy"></p>
<p>好了，现在你知道了 Policy 和 Value，我们就知道如何来选取 Policy 了。通过比较每个 Policy 下的 Value Function，看看 Agent 遵循哪个 Policy 能获得最大的 Return，这个 Policy 就是 Optimal Policy。这也就是强化学习的目标了。</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/OptimalPolicy.png" alt="OptimalPolicy" loading="lazy"></p>
<h2 id="bellman-equation"><a class="markdownIt-Anchor" href="#bellman-equation">#</a> Bellman Equation</h2>
<p>（Tips：我们讨论的 RL 绝大多数都是 Model-Free Control problems，具体内容可参见文章顶部连接。）</p>
<p>贝尔曼方程是所有 RL 算法的基础，接下来我将用 R 表示 immediate (observed) Reward，用 G 表示 Return 来了解一下贝尔曼方程</p>
<p>我们假设 MDP 最后一个状态为 S8，从状态 S7 到状态 S8 获得的 Reward 为 R8（个人觉得写 R7 会更好），按照前文所述，那么 S7 的 Return 就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>7</mn><mo>=</mo><mi>R</mi><mn>8</mn></mrow><annotation encoding="application/x-tex">G7=R8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span><span class="mord">7</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord">8</span></span></span></span></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Bellman1.jpg" alt="Bellman1" loading="lazy"></p>
<p>接下来看看状态 S6 是个什么情况。状态 S6 的 Return G6 包含两部分：一是 S6 到 S7 的 Reward R6，二是 S7 的 Return G7 乘以折扣因子。即：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mn>6</mn><mo>=</mo><mi>R</mi><mn>6</mn><mo>+</mo><mi>γ</mi><mi>G</mi><mn>7</mn></mrow><annotation encoding="application/x-tex">G6=R6+\gamma G7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">G</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mord">6</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord mathnormal">G</span><span class="mord">7</span></span></span></span></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Bellman2.jpg" alt="Bellman2" loading="lazy"></p>
<p>这样我们就得到了一个递归方程，即贝尔曼方程：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mi>γ</mi><msub><mi>G</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">G_t=R_t+\gamma G_{t+1}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathnormal">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span></span></p>
<p>如果我们用 Q-Value 来表示这个方程的话：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="double-struck">E</mi><mo stretchy="false">[</mo><msub><mi>R</mi><mi>t</mi></msub><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>a</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">Q(S_t,a_t) = \mathbb{E}[R_t + \gamma Q(S_{t+1},a_{t+1})]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathbb">E</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Bellman3.png" alt="Bellman3" loading="lazy"></p>
<p>贝尔曼方程为何如此有用？一是因为我们可以递归地计算 Return，只需要采取一步 Action 即可，而不要遍历完 episode；其次，我们前文说到过 Value 是一个预期值，是需要修正的，那修正的方法就是通过贝尔曼方程了。我们计算已有的 Value 和计算得到的 Value 之间的误差，也就是 error，来改进我们的预测值。</p>
<h2 id="model-free-solutions"><a class="markdownIt-Anchor" href="#model-free-solutions">#</a> Model-free Solutions</h2>
<p>好了，现在我们要做的就是如何去找 Optimal Policy。在前文已经说过，Optimal Policy 对应着最大的 Q-Value，也即 Optimal Q-Value。那么我们找到 Optimal Q-Value 也是可以的，因为它对应这 Optimal Policy。这样我们就有两种算法：Policy-based 和 Value-based</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/policy-value-based.jpg" alt="policy-value-based" loading="lazy"></p>
<p>Policy-based 算法是直接寻找 Optimal Policy，而 Value-based 算法是先找到 Optimal Q-Value，然后 Optimal Policy 从中可以推断出来。如何推断？某状态下采取哪个 Action 获得的 Q 值最大，那么采取这个 Action 的概率就为 1（但随机最优策略是有必要的）</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/value-based.jpg" alt="Value-based" loading="lazy"></p>
<p>常见 Model-free 算法可分为以下分类：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/modelfree.png" alt="modelfree" loading="lazy"></p>
<p>我们来重点关注一下解决这类问题的 4 大步骤：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/RLSolution.png" alt="RLSolution" loading="lazy"></p>
<p><strong>1.Initialize estimates</strong></p>
<p>随机初始化估计值，Q-Value 全初始为 0</p>
<p><strong>2.Take an Action</strong></p>
<p>Agent 想要确保他尝试了所有的途径，找到最佳的一个。这是如何做到的？我们要了解 Exploration 和 Exploitation。</p>
<p>**Exploration——** 在刚开始学习时，我们并不知道哪个 Action 是更好的，所以我们要随机的选取 Action 观察 Reward</p>
<p>**Exploitation——** 当 Agent 被充分训练后，我们已经探索过了所有可能的途径，所以我们选取能产生最大 Return 的 Action</p>
<p>Agent 要平衡好这两者之间的程度。对于 Policy-based 算法，我们用自己的估计概率去选取 Action 就好：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/policypick.jpg" alt="policypick" loading="lazy"></p>
<p>对于 Value-based 算法，我们采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>−</mo><mi>g</mi><mi>r</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>y</mi></mrow><annotation encoding="application/x-tex">\epsilon-greedy</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">e</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 策略选取 Action：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/greedy.jpg" alt="greedy" loading="lazy"></p>
<p>这是一个动态策略，首先我们初始<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\epsilon=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>，让它以某一速率随着迭代次数的增加而衰减。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 代表随机选取 Action 的概率，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> 代表选取有最大 Q-Value 的 Action 的概率。这就实现了在训练前期更多的 Exploration，在训练后期更多的 Exploitation。</p>
<p><strong>3.Get feedback from the environment</strong></p>
<p>Agent 从当前状态 S 采取 Action，然后从环境中获得了 Reward，并且自己到达了下一个状态 S’。我们从环境中观测到的数据 observation data 表示为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>R</mi><mo separator="true">,</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S,a,R,S&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p><strong>4.Improve estimates</strong></p>
<p>对于 Policy-based 算法，如果 Agent 采取了 Action 之后获得的 Reward 是 “正奖励”，即 postive，那么就增加刚刚选取的 Action 的概率</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/PolicyImprove.png" alt="PolicyImprove" loading="lazy"></p>
<p Error="">对于 Value-based 算法，我们通过 Bellman Equation 来更新 Q-Value。我们计算方程得出的结果与自己的预测值之间的误差，来改进估计值。误差计算为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>TD Error</mtext><mo>=</mo><mo stretchy="false">(</mo><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><mi>γ</mi><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>3</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><msub><mi>S</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>a</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{TD Error}=(R_1 + \gamma Q(S_3,a_3))-Q(S_1,a_1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">TD Error</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，更新公式为Q(S_1,a_1)=Q(S_1,a_1)+\alpha * \text</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/ValueImprove.png" alt="ValueImprove" loading="lazy"></p>
<p>我们可以看看以上四个步骤的总览：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/overview.png" alt="overview" loading="lazy"></p>
<p>算法的核心在于如何改进估计，不同的算法有着细微的差别。我们大致可以从以下三个方面来分辨：</p>
<ul>
<li>**Frequency—**Agent 每进行多少 steps 去更新估计</li>
<li>**Depth—** 当发生更新时，往回更新多少 steps 的估计（propagate）</li>
<li>**Formula—** 计算 updated estimates 的方式</li>
</ul>
<p>现在我们来了解一下这三个方面的内涵</p>
<p><strong>Frequency</strong></p>
<ul>
<li>**Episode—**Agent 每采取一次 Action，获得 Reward 并且储存它们。在每一次迭代周期的最后，算法利用这些 Reward 来更新估计</li>
<li>**One Step—**Agent 采取 Action 后获得 Reward，然后马上进行更新，再进行下一个 step，而不是等到迭代结束</li>
<li>**N steps—** 介于上述两种方式之间，每隔 N steps 更新</li>
</ul>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Frequency.png" alt="Frequency" loading="lazy"></p>
<p><strong>Depth</strong></p>
<ul>
<li>**Episode—**Agent 向前步进直到迭代结束，算法更新 Agent 沿路上所有的估计（state-action pairs）</li>
<li>**One Step—** 只会更新当前的估计</li>
<li>**N Steps—** 介于上述两种方式之间，更新沿路上 N steps 的估计</li>
</ul>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Depth.png" alt="Depth" loading="lazy"></p>
<p><strong>Update formula</strong></p>
<ul>
<li>Value-based 用 Bellman Equation 来更新 Q-Value，这里用到了 TD Error</li>
<li>Policy-based 根据 Agent 收获的 Reward 是否是 positive 来增加或减少选取概率</li>
</ul>
<p>这里有个总结：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/conclusion.png" alt="conclusion" loading="lazy"></p>
<h2 id="q-learning"><a class="markdownIt-Anchor" href="#q-learning">#</a> Q-Learning</h2>
<p>Q-Learning 算法基于 Q-table，行为 states，列为 actions，表中的值为 Q-Value。</p>
<p>①首先初始化 Q-table，值全部初始化为 0 值。</p>
<p>②我们假设某时刻有如下的 Q-table：</p>
<table>
<thead>
<tr>
<th></th>
<th>a1</th>
<th>a2</th>
<th>a3</th>
<th>a4</th>
</tr>
</thead>
<tbody>
<tr>
<td>S1</td>
<td>4</td>
<td>9</td>
<td>2</td>
<td>3</td>
</tr>
<tr>
<td>S2</td>
<td>0</td>
<td>3</td>
<td>4</td>
<td>7</td>
</tr>
<tr>
<td>S3</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
<p>假如 Agent 现在的状态为 S1，Agent 采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>−</mo><mtext>greedy policy</mtext></mrow><annotation encoding="application/x-tex">\epsilon-\text{greedy policy}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">greedy policy</span></span></span></span></span> 从 current state (S1) 选取 current action (假设为 a1)。然后 Agent 执行 a1 与环境进行交互，并从环境中得到反馈 Reward (R1) 和 next state (我们假设为 S2)</p>
<p>③Q-Learning 会从 S2 中选取一个 Q-Value 来更新 current state (S1) 和 selected action (a1) 相对应的 estimated Q-Value (Q (S1,a1))。那么在 S2 中应该选取哪个 Action 呢？我们选取具有最高 Q-Value 的动作，即 a4，我们用 7 来更新 Q (S1,a1)。<strong>这里请注意</strong>，a4 仅仅是用来更新 Q (S1,a1) 的，当 Agent 更新完后来到 S2 时并不一定要选取 a4，a4 又被称作为 &quot;target action&quot;</p>
<p>④现在我们找到了 target Q-Value (Q (S2,a4)=7)，我们用前文所述的公式来更新 Q (S1,a1)</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>Q</mi><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo stretchy="false">)</mo><mo>+</mo><mi>α</mi><mo stretchy="false">(</mo><mi>R</mi><mo>+</mo><mi>γ</mi><mi>max</mi><mo>⁡</mo><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo>−</mo><mi>Q</mi><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>A</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Q(S,A)=Q(S,A)+\alpha(R+\gamma \max Q(S&#x27;,a)-Q(S,A))
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span> 为学习速率，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span> 为折扣因子。相信有部分人和我一样当初看到这个式子的时候一脸蒙，现在你应该理解了。</p>
<p>这神奇的 Q-Learing 为什么会看似在估计对估计的改进中收敛趋于最优策略？因为我们的 Q 值是包含实际的 Reward，这个不是胡乱估计的，经过多次迭代我们就会找到产生最大 Return 的一个策略。</p>
<blockquote>
<p>​	Q-Learning 又被称为 &quot;off-policy&quot; learning，因为 Agent 实际采取的 Action 与用于学习的 target Action 不相同</p>
</blockquote>
<h2 id="deep-q-networks"><a class="markdownIt-Anchor" href="#deep-q-networks">#</a> Deep Q Networks</h2>
<p>在 Q-Learning 中，我们的架构大概是这个样子：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Q-Learning.png" alt="Q-Learning" loading="lazy"></p>
<p>但是在现实世界中，状态数和动作数实在是太多了，我们几乎不可能用一张表去记录下它们。我们应该用一个函数 Q-Function 来实现这个映射关系：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/Q-Function.png" alt="Q-Function" loading="lazy"></p>
<p>正好，神经网络非常适合用来建模复杂的函数，我们叫它 Deep Q Networks (DQN)。我们训练神经网络的参数来让它生成最优的 Q-Values</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN1.png" alt="DQN1" loading="lazy"></p>
<p>DQN 架构中有三个组成部分：Experience Replay、Q Network、Target Network</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN2.png" alt="DQN2" loading="lazy"></p>
<p>Experience Replay 用于与环境互动，生成数据并训练 Q Network。Q Network 中的 Agent 被训练用来产生准确的 Q-Value，神经网络是简单的线性网络，如果你的 state 数据是图像或文字，就得用到 CNN 和 RNN。Target Network 和 Q Network 结构相同，但不被训练，后面会详细讲到。</p>
<p>我们来看看详细的流程图：</p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN3.png" alt="DQN3" loading="lazy"></p>
<p><strong>①Experience Replay 收集训练数据</strong></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN0.png" alt="DQN0" loading="lazy"></p>
<p>Agent 在 current state 下采用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi><mo>−</mo><mtext>greedy policy</mtext></mrow><annotation encoding="application/x-tex">\epsilon-\text{greedy policy}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">greedy policy</span></span></span></span></span> 选择 Action，然后与环境交互，得到反馈 Reward 和 next state，将此次 observation 作为 training data 的一个样本，并将它存放在记忆库中。所以一项训练数据包含四个要素：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mi>R</mi><mo separator="true">,</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S,a,R,S&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></p>
<p><strong>②Q Network predicts Q-value</strong></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN4.png" alt="DQN4" loading="lazy"></p>
<p>从记忆库中随机抽取一批训练数据（批训练），它们包含了最近或很久以前的数据。将这些数据传入神经网络，Q Network 用其中的 current state 和 action 去预测 Q-Value。这被称作 &quot;Predicted Q Value&quot;</p>
<p><strong>③Target Q Network predits Target Q-Value</strong></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN5.png" alt="DQN5" loading="lazy"></p>
<p>Target Network 用数据中的 next state 去预测出这个状态下所有 actions 中值最高的 best Q-Value，并且用它来计算 &quot;Target Q-Value&quot;</p>
<p><strong>④计算误差并训练网络参数</strong></p>
<p><img src="https://gitee.com/tzh363231879/picgo/raw/master/DQN6.png" alt="DQN6" loading="lazy"></p>
<p>Predicted Q-Value、best Q-Value 和 reward 用于计算 Loss 并训练 Q Network，但是 Target Network 不被训练（切断相关性，为了使 Target Q-Value 是稳定的，不然就会像追寻一个变化的目标）</p>
<p>Target Q-Value 的计算如下：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Target Q-Value</mtext><mo>=</mo><mi>R</mi><mo>+</mo><mi>γ</mi><mo>∗</mo><mi>max</mi><mo>⁡</mo><mi>Q</mi><mo stretchy="false">(</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Target Q-Value}=R+\gamma * \max Q(S&#x27;)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">Target Q-Value</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.6597200000000001em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="mop">max</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>Loss 的计算如下：</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Loss</mtext><mo>=</mo><mtext>MSE</mtext><mo stretchy="false">(</mo><mtext>Predicted Q Value</mtext><mo separator="true">,</mo><mtext>Target Q Value</mtext><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mo>∑</mo><mo stretchy="false">(</mo><mtext>Predict</mtext><mo>−</mo><mtext>Target</mtext><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><mtext>batch size</mtext></mfrac></mrow><annotation encoding="application/x-tex">\text{Loss}=\text{MSE}(\text{Predicted Q Value},\text{Target Q Value})=\cfrac{\sum(\text{Predict}-\text{Target})^2}{\text{batch size}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord text"><span class="mord">Loss</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">MSE</span></span><span class="mopen">(</span><span class="mord text"><span class="mord">Predicted Q Value</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">Target Q Value</span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.276em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord text"><span class="mord">batch size</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="mopen">(</span><span class="mord text"><span class="mord">Predict</span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord text"><span class="mord">Target</span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p>
<p><strong>⑤每经过 T 次迭代，将 Q Network 的参数同步给 Target Network，以预测出更加准确的 Q-Value</strong></p>
</div><div id="reward-container"><span class="hty-icon-button button-glow" id="reward-button" title="打赏" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === &quot;none&quot;) ? &quot;block&quot; : &quot;none&quot;;"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-hand-coin-line"></use></svg></span><div id="reward-comment">「真诚赞赏，手留余香」</div><div id="qr" style="display:none;"><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://gitee.com/tzh363231879/picgo/raw/master/支付宝.jpg"><img loading="lazy" src="https://gitee.com/tzh363231879/picgo/raw/master/支付宝.jpg" alt="支付宝" title="支付宝"></a><div><span style="color:#00A3EE"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-alipay-line"></use></svg></span></div></div><div style="display:inline-block"><a target="_blank" rel="noopener" href="https://gitee.com/tzh363231879/picgo/raw/master/微信.png"><img loading="lazy" src="https://gitee.com/tzh363231879/picgo/raw/master/微信.png" alt="微信支付" title="微信支付"></a><div><span style="color:#2DC100"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wechat-pay-line"></use></svg></span></div></div></div></div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者：</strong>2bW</li><li class="post-copyright-link"><strong>本文链接：</strong><a href="https://2bwant2b.github.io/2022/04/05/%E5%8D%9A%E5%AE%A2/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3DRL/" title="一文带你了解DRL">https://2bwant2b.github.io/2022/04/05/%E5%8D%9A%E5%AE%A2/%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3DRL/</a></li><li class="post-copyright-license"><strong>版权声明：</strong>本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 "><svg class="icon"><use xlink:href="#icon-creative-commons-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-by-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-nc-line"></use></svg><svg class="icon"><use xlink:href="#icon-creative-commons-sa-line"></use></svg></a> 许可协议。</li></ul></section></article><div class="post-nav"><div class="post-nav-item"></div><div class="post-nav-item"><a class="post-nav-next" href="/2022/01/26/%E5%8D%9A%E5%AE%A2/%E8%80%83%E7%A0%94/2023%E6%AD%A6%E5%BF%A0%E7%A5%A5%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/" rel="next" title="2023武忠祥高等数学基础"><span class="post-nav-text">2023武忠祥高等数学基础</span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-right-s-line"></use></svg></a></div></div></div><div class="hty-card" id="comment"><div class="comment-tooltip text-center"><span>留下你想说的话吧</span><br></div><div id="valine-container"></div><script>Yun.utils.getScript("https://cdn.jsdelivr.net/npm/valine@latest/dist/Valine.min.js", () => {
  const valineConfig = {"enable":true,"appId":"P2B8WEYNUAQ0z68tTomJdPbP-MdYXbMMI","appKey":"iVlRH4IjcvkkJeAw7Eq2NuvU","placeholder":"开始写吧","avatar":null,"pageSize":10,"visitor":false,"highlight":true,"recordIP":false,"enableQQ":true,"meta":["nick","mail","link"],"el":"#valine-container","lang":"zh-cn"}
  valineConfig.path = window.location.pathname
  new Valine(valineConfig)
}, window.Valine);</script></div></main><footer class="sidebar-translate" id="footer"><div class="beian"><a rel="noopener" href="https://beian.miit.gov.cn/" target="_blank">湘ICP备2021002467号</a></div><div class="copyright"><span>&copy; 2021 – 2022 </span><span class="with-love" id="animate"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-heart-line"></use></svg></span><span class="author"> 2bW</span></div><div class="live_time"><span>本博客已萌萌哒地运行</span><span id="display_live_time"></span><span class="moe-text">(●'◡'●)❤(◍•ᴗ•◍)</span><script>function blog_live_time() {
  setTimeout(blog_live_time, 1000);
  const start = new Date('2021-02-03T08:05:20');
  const now = new Date();
  const timeDiff = (now.getTime() - start.getTime());
  const msPerMinute = 60 * 1000;
  const msPerHour = 60 * msPerMinute;
  const msPerDay = 24 * msPerHour;
  const passDay = Math.floor(timeDiff / msPerDay);
  const passHour = Math.floor((timeDiff % msPerDay) / 60 / 60 / 1000);
  const passMinute = Math.floor((timeDiff % msPerHour) / 60 / 1000);
  const passSecond = Math.floor((timeDiff % msPerMinute) / 1000);
  display_live_time.innerHTML = " " + passDay + " 天 " + passHour + " 小时 " + passMinute + " 分 " + passSecond + " 秒";
}
blog_live_time();
</script></div><div id="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv" title="总访客量"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-user-line"></use></svg></span><span id="busuanzi_value_site_uv"></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv" title="总访问量"><span><svg class="icon" aria-hidden="true"><use xlink:href="#icon-eye-line"></use></svg></span><span id="busuanzi_value_site_pv"></span></span></div><div class="footer-custom-text"><a target="_blank" rel="noopener" href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral"><img src="https://gitee.com/tzh363231879/picgo/raw/master/又拍云_logo5.png"align="absmiddle" width="59px" height="30px" /></a></div></footer><a class="hty-icon-button" id="goUp" aria-label="back-to-top" href="#"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-arrow-up-s-line"></use></svg><svg class="progress-circle-container" viewBox="0 0 100 100"><circle class="progress-circle" id="progressCircle" cx="50" cy="50" r="48" fill="none" stroke="#0078E7" stroke-width="2" stroke-linecap="round"></circle></svg></a><a class="popup-trigger hty-icon-button icon-search" id="search" href="javascript:;" title="搜索"><span class="site-state-item-icon"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-search-line"></use></svg></span></a><script>window.addEventListener("DOMContentLoaded", () => {
  // Handle and trigger popup window
  document.querySelector(".popup-trigger").addEventListener("click", () => {
    document.querySelector(".popup").classList.add("show");
    setTimeout(() => {
      document.querySelector(".search-input").focus();
    }, 100);
  });

  // Monitor main search box
  const onPopupClose = () => {
    document.querySelector(".popup").classList.remove("show");
  };

  document.querySelector(".popup-btn-close").addEventListener("click", () => {
    onPopupClose();
  });

  window.addEventListener("keyup", event => {
    if (event.key === "Escape") {
      onPopupClose();
    }
  });
});
</script><script src="/js/search/local-search.js" defer></script><div class="popup search-popup"><div class="search-header"><span class="popup-btn-close close-icon hty-icon-button"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-close-line"></use></svg></span></div><div class="search-input-container"><input class="search-input" id="local-search-input" type="text" placeholder="搜索..." value=""></div><div id="local-search-result"></div></div><script>const date = new Date();
const today = (date.getMonth() + 1) + "-" + date.getDate()
const mourn_days = ["4-4","9-18"]
if (mourn_days.includes(today)) {
  document.documentElement.style.filter = "grayscale(1)";
}</script></div><!-- hexo injector body_end start --><script src="https://cdn.jsdelivr.net/npm/hexo-tag-common@latest/js/index.js"></script><!-- hexo injector body_end end --><script src="/live2d/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2d/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"display":{"position":"right"},"mobile":{"show":false},"model":{"scale":1,"jsonPath":"/live2d/assets/weier.model.json"},"dialog":{"enable":true},"log":false});</script></body></html>